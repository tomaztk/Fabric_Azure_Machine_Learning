{"cells":[{"cell_type":"markdown","source":["# Module 5 - Excercise overview\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ee6dc8a8-ee2a-4a31-a115-6c0b9097c50e"},{"cell_type":"markdown","source":["### Fabric Prerequistis\n","\n","You need to have Lakehouse enabled and connected. \n","\n","Link to Lakehouse (replace these strings)\n","- Tables: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables`\n","- Files: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Files`\n","\n","You will also need:\n","- PySpark notebook and connect it to the Fabric standard session"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"268b4cde-27fa-4930-ae15-dc6d5dea5ecd"},{"cell_type":"markdown","source":["## Exercise 1: Test Model Predictions and Perform Inference Using PySpark"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"49359dc2-27fd-4295-ab86-e00ba75ed625"},{"cell_type":"markdown","source":["In this exercise, you will test the predictions of a previously trained machine learning model and perform inference on new data using PySpark.\n","Step-by-Step Instructions:\n","\n","1) Load the Trained Model:\n","    - In Microsoft Fabric, load the trained model from the previous exercise (e.g., a saved Linear Regression model) using PySpark.\n","\n","2) Load New Data for Inference:\n","    - Load new data into the notebook, which the model will use for inference (i.e., predicting values based on unseen data).\n","\n","3) Perform Inference and Evaluate Predictions:\n","    - Use the model to generate predictions on the new data, and evaluate the predictions using metrics like RMSE (Root Mean Squared Error) and R-squared."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0355deff-86d9-4595-9b01-dae02a8cd2b1"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.regression import LinearRegressionModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ModelInference\").getOrCreate()\n","\n","# Step 1: Load the trained Linear Regression model from disk (assuming it was saved earlier)\n","model_path = \"path/to/saved_model\"\n","lr_model = LinearRegressionModel.load(model_path)\n","\n","# Step 2: Load new data for inference (Assuming new sales data)\n","new_data_path = \"Files/Users/new_sales_data.csv\"\n","new_df = spark.read.csv(new_data_path, header=True, inferSchema=True)\n","\n","# Feature engineering on new data (assuming same transformations as in training)\n","from pyspark.sql.functions import year, month\n","new_df = new_df.withColumn(\"year\", year(col(\"date\"))).withColumn(\"month\", month(col(\"date\")))\n","\n","# Assuming feature vector is created (similar to how it was done during model training)\n","from pyspark.ml.feature import VectorAssembler\n","assembler = VectorAssembler(inputCols=[\"year\", \"month\", \"region_encoded\"], outputCol=\"features\")\n","new_df_transformed = assembler.transform(new_df)\n","\n","# Step 3: Perform Inference (Generate predictions)\n","predictions = lr_model.transform(new_df_transformed)\n","\n","# Step 4: Evaluate predictions using RegressionEvaluator\n","evaluator = RegressionEvaluator(labelCol=\"sales_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n","rmse = evaluator.evaluate(predictions)\n","r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n","\n","print(f\"Root Mean Squared Error (RMSE) on new data: {rmse}\")\n","print(f\"R-squared on new data: {r2}\")\n","\n","# Show sample predictions\n","predictions.select(\"features\", \"sales_amount\", \"prediction\").show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1757b2a-bc98-468c-8c2c-bdfc52d9d223"},{"cell_type":"markdown","source":["## Exercise 2: Job Scheduling and Running Jobs for Model Inference"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"045cabd3-4936-4869-ae64-6310daff0ca5"},{"cell_type":"markdown","source":["In this exercise, you will automate the inference process by scheduling a job to run the inference notebook periodically using Microsoft Fabricâ€™s job scheduling capabilities.\n","Step-by-Step Instructions:\n","\n","1) Create a Notebook for Inference:\n","    - Create a notebook that loads the trained model, performs inference on new data, and saves the predictions to a Lakehouse.\n","\n","2) Schedule the Job in Microsoft Fabric:\n","    - Use the Job Scheduling feature in Microsoft Fabric to schedule the notebook for periodic execution (e.g., daily or weekly).\n","\n","3) Monitor and Manage Scheduled Jobs:\n","    - Monitor the job's performance, check logs, and ensure the notebook is running successfully at scheduled intervals."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0da56250-8777-4e90-b8df-ce33e145df1e"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.regression import LinearRegressionModel\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ScheduledModelInference\").getOrCreate()\n","\n","# Step 1: Load the trained model\n","model_path = \"path/to/saved_model\"\n","lr_model = LinearRegressionModel.load(model_path)\n","\n","# Step 2: Load new data for inference\n","new_data_path = \"Files/Users/new_sales_data.csv\"\n","new_df = spark.read.csv(new_data_path, header=True, inferSchema=True)\n","\n","# Perform same feature engineering as training\n","from pyspark.sql.functions import year, month\n","new_df = new_df.withColumn(\"year\", year(col(\"date\"))).withColumn(\"month\", month(col(\"date\")))\n","\n","# Assemble features for prediction\n","from pyspark.ml.feature import VectorAssembler\n","assembler = VectorAssembler(inputCols=[\"year\", \"month\", \"region_encoded\"], outputCol=\"features\")\n","new_df_transformed = assembler.transform(new_df)\n","\n","# Perform inference\n","predictions = lr_model.transform(new_df_transformed)\n","\n","# Save the predictions to the Lakehouse for further analysis\n","predictions.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/Predictions_Lakehouse\")\n","\n","# Print confirmation\n","print(\"Scheduled job executed successfully, predictions saved to Lakehouse.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f0bf84b-5503-4cd7-af26-efcef3ef6d6b"},{"cell_type":"markdown","source":["## Exercise 3: Model Retraining Using MLflow and Job Scheduling"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"afabb406-6a23-4551-8b8b-235c866e7a12"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}