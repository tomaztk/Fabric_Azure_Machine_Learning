{"cells":[{"cell_type":"markdown","source":["# Module3  - Excercise overview"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba70edf9-e010-4ba7-a4e1-e0f996e92dc3"},{"cell_type":"markdown","source":["### Fabric Prerequistis\n","\n","You need to have Lakehouse enabled and connected. \n","\n","Link to Lakehouse (replace these strings)\n","- Tables: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables`\n","- Files: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Files`\n","\n","You will also need:\n","- PySpark notebook and connect it to the Fabric standard session\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17ba0876-0684-4835-ba31-8aacb73e503d"},{"cell_type":"markdown","source":["# Exercise 1: Data Movement Using Shortcuts from One Lakehouse to Another"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bd2ff702-f067-4248-9729-e8a44c5d26e7"},{"cell_type":"markdown","source":["In this exercise, you will move data from one Lakehouse to another using Shortcuts in Microsoft Fabric. Shortcuts allow you to connect data from different storage locations without needing to duplicate the data.\n","Step-by-Step Instructions:\n","\n","1) Create Two Lakehouses:\n","    - In Microsoft Fabric, create two Lakehouses (e.g., Source_Lakehouse and Target_Lakehouse) from the \"New\" menu.\n","    - The Source_Lakehouse will contain the data you want to move, and the Target_Lakehouse will be where you place the moved data.\n","\n","2) Create a Shortcut to the Source Lakehouse:\n","    - In the Target_Lakehouse, create a Shortcut to the Source_Lakehouse. Shortcuts allow you to access data from one Lakehouse without physically copying it.\n","\n","3) Move Data Using PySpark:\n","    - Using a notebook, read the data from the Shortcut and write it into the Target_Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26225c89-52e2-464c-9bdd-ed9ed2e7b46b"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DataMovement\").getOrCreate()\n","\n","# Path to the shortcut created in the Target Lakehouse pointing to Source Lakehouse\n","source_shortcut_path = \"Tables/Shortcut_to_Source_Lakehouse\"\n","\n","# Load data from the Source Lakehouse via the shortcut\n","df = spark.read.format(\"delta\").load(source_shortcut_path)\n","\n","# Show the data loaded from the Source Lakehouse\n","df.show()\n","\n","# Write the data to the Target Lakehouse as a Delta table\n","target_lakehouse_path = \"Tables/Target_Lakehouse_Data\"\n","df.write.format(\"delta\").mode(\"overwrite\").save(target_lakehouse_path)\n","\n","print(\"Data successfully moved from Source Lakehouse to Target Lakehouse.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9aa59e70-45f6-4a73-b568-c6696ec33737"},{"cell_type":"markdown","source":["## Exercise 2: Creating a Data Pipeline for Extract-Transform-Load (ETL) in Microsoft Fabric "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f0b2ca3b-af3d-44c2-8596-17dbfd07f861"},{"cell_type":"markdown","source":["In this exercise, you will create a data pipeline that extracts data from a data source, performs transformation, and loads it into a target storage (Lakehouse). You will be using Dataflows Gen2 to automate the process and notebooks to perform custom transformations.\n","Step-by-Step Instructions:\n","\n","1) Create a Dataflow:\n","    - In Microsoft Fabric, create a Dataflow Gen2 for the ETL pipeline.\n","    - Set the data source as an external database, file, or any other available data sources (e.g., SQL database, CSV file).\n","\n","2) Extract Data Using a PySpark Notebook:\n","    - Use a PySpark notebook to extract the data from the source and load it into the Lakehouse.\n","\n","3) Transform Data:\n","    - Perform transformations using the notebook (e.g., filtering, aggregating, and cleaning data).\n","\n","4) Load Transformed Data into Lakehouse:\n","    - Finally, load the transformed data into a Delta Table in the Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4690138c-b108-4fab-b5eb-864f459412ae"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ETL_Pipeline\").getOrCreate()\n","\n","# Step 1: Extract Data (Assuming source is a CSV file in Data Lake)\n","source_data_path = \"Files/source_data.csv\"\n","df = spark.read.csv(source_data_path, header=True, inferSchema=True)\n","\n","# Step 2: Transform Data (Example: Filter out rows where 'age' is less than 30)\n","transformed_df = df.filter(df['age'] >= 30)\n","\n","# Step 3: Load Transformed Data into Lakehouse Delta Table\n","target_table_path = \"Tables/Transformed_Data\"\n","transformed_df.write.format(\"delta\").mode(\"overwrite\").save(target_table_path)\n","\n","print(\"ETL pipeline executed successfully. Data loaded into Lakehouse.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be763bd5-bc38-4e6d-993e-e1c495e508cb"},{"cell_type":"markdown","source":["## Exercise 3: Data Transformation and Aggregation in a Data Lake Using PySpark"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"85b10b84-839e-46a9-958f-abbe1862cbff"},{"cell_type":"markdown","source":["In this exercise, you will work with large datasets in a Data Lake, performing transformations such as aggregation, filtering, and creating summary tables. You'll use PySpark to load, transform, and store data back in the Data Lake for further analysis.\n","Step-by-Step Instructions:\n","\n","1) Access Data from the Data Lake:\n","    - In Microsoft Fabric, connect to your Data Lake (OneLake).\n","    - Use PySpark to read a dataset stored in the Data Lake.\n","\n","2) Perform Data Transformations:\n","    - Apply transformations such as filtering data, performing group-by operations, and calculating summary statistics (e.g., average, sum).\n","\n","3) Write the Aggregated Data Back to the Data Lake:\n","    - Save the aggregated/summary data back to the Data Lake in Delta format for further analysis or reporting."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ca0592c-423c-4cb8-97e4-1806ff727ed9"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DataLake_Transformation\").getOrCreate()\n","\n","# Step 1: Load Data from Data Lake\n","data_lake_path = \"Files/Users/data_lake_sales_data\"\n","sales_df = spark.read.format(\"delta\").load(data_lake_path)\n","\n","# Step 2: Perform Transformations (Group by 'region' and calculate total sales and average sales)\n","aggregated_df = sales_df.groupBy(\"region\").agg(\n","    {\"sales_amount\": \"sum\", \"sales_amount\": \"avg\"}\n",")\n","\n","# Rename the columns for clarity\n","aggregated_df = aggregated_df.withColumnRenamed(\"sum(sales_amount)\", \"total_sales\") \\\n","                             .withColumnRenamed(\"avg(sales_amount)\", \"average_sales\")\n","\n","# Show the result\n","aggregated_df.show()\n","\n","# Step 3: Save the aggregated data back to the Data Lake\n","output_path = \"Files/Users/aggregated_sales_data\" # Change the path to your datalakes\n","aggregated_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n","\n","print(\"Aggregated data successfully written to Data Lake.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"150a869d-68af-4083-b4fe-4218e25b8cd4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}