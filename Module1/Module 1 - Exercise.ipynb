{"cells":[{"cell_type":"markdown","source":["# Module 1 - Exercise\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"908b124b-8014-4cae-b922-e8a52d5c4100"},{"cell_type":"markdown","source":["## Exercise 1: Navigate the Lakehouse and Read Data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"93fd96b8-d1c6-4fb8-b6b5-4c21d7692a91"},{"cell_type":"markdown","source":["In this exercise, we will explore how to connect to a Lakehouse in Microsoft Fabric and perform a basic data read operation using PySpark.\n","Steps:\n","\n","1) Create a Lakehouse: In Microsoft Fabric, create a Lakehouse to store structured and unstructured data.\n","2) Load Data: Assume you have ingested data into your Lakehouse (e.g., California Housing Prices).\n","2) Use PySpark: Load the data from the Delta Table format in your Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"accc6d09-a895-4780-980c-196d6758ef67"},{"cell_type":"code","source":["# Step 1: Set up the Spark session\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"Fabric_Lakehouse_Demo\").getOrCreate()\n","\n","# Step 2: Define the Lakehouse table path (Delta Table)\n","lakehouse_table_path = \"abfss://<your-container>@<your-storage-account>.dfs.core.windows.net/delta/your-lakehouse-table\"\n","\n","# Step 3: Read the data from the Delta table\n","df = spark.read.format(\"delta\").load(lakehouse_table_path)\n","\n","# Step 4: Display the first 5 rows of the dataset\n","df.show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7cac6a79-c07b-4057-ab09-1cc835ff5fca"},{"cell_type":"markdown","source":["## Exercise 2: Create a Simple Data Pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6d6a05f-0720-4d11-a4de-f95ab228047e"},{"cell_type":"markdown","source":["In this exercise, you will create a basic Data Pipeline in Microsoft Fabric using Notebooks and Pipelines. You will extract data from a source, transform it using PySpark, and then load it back into a Lakehouse.\n","\n","Steps:\n","\n","1) Create a Data Pipeline: Set up a data pipeline in Microsoft Fabric.\n","2) Extract: Read data from an external source (e.g., CSV or database) and load it into the Data Lake.\n","3) Transform: Perform some simple data transformation using PySpark (e.g., filtering and aggregation).\n","4) Load: Write the transformed data back to your Lakehouse in Delta format."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bdec97f2-bba2-48d8-9ddb-863bdfa1dca8"},{"cell_type":"code","source":["# Step 1: Read data from a source (e.g., CSV)\n","source_path = \"abfss://<your-container>@<your-storage-account>.dfs.core.windows.net/raw/california_housing.csv\"\n","df = spark.read.csv(source_path, header=True, inferSchema=True)\n","\n","# Step 2: Perform transformation (e.g., Filter rows where 'median_house_value' > 200000)\n","filtered_df = df.filter(df[\"median_house_value\"] > 200000)\n","\n","# Step 3: Write the transformed data back to the Lakehouse as Delta Table\n","filtered_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://<your-container>@<your-storage-account>.dfs.core.windows.net/delta/transformed_california_housing\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a85430a1-9e6c-4a23-9e67-0170436cbff1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}