{"cells":[{"cell_type":"markdown","source":["# Module 1 - Exercise\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"908b124b-8014-4cae-b922-e8a52d5c4100"},{"cell_type":"markdown","source":["## Exercise 1: Navigate the Lakehouse and Read Data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"93fd96b8-d1c6-4fb8-b6b5-4c21d7692a91"},{"cell_type":"markdown","source":["In this exercise, we will explore how to connect to a Lakehouse in Microsoft Fabric and perform a basic data read operation using PySpark.\n","Steps:\n","\n","1) Create a Lakehouse: In Microsoft Fabric, create a Lakehouse to store structured and unstructured data.\n","2) Load Data: Assume you have ingested data into your Lakehouse (e.g., California Housing Prices).\n","2) Use PySpark: Load the data from the Delta Table format in your Lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"accc6d09-a895-4780-980c-196d6758ef67"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","\n","\n","spark = SparkSession.builder.appName(\"Fabric_Lakehouse_Upload_file\").getOrCreate()\n","\n","\n","data = [\n","    Row(id=1, name='John Doe', age=100, city='New York'),\n","    Row(id=2, name='Enrico van de Laar', age=42, city='Utrecht'),\n","    Row(id=3, name='Tomaz Kastrun', age=45, city='Ljubljana')\n","]\n","df = spark.createDataFrame(data)\n","\n","# Write DataFrame to Delta Table in the Lakehouse\n","lakehouse_path = \"abfss://fabric@onelake.fabric/Lake.Lakehouse/Tables/Sample\"  # Modify path according to your Lakehouse structure\n","df.write.format(\"delta\").mode(\"overwrite\").save(lakehouse_path)\n","\n","print(\"Data successfully written to the Delta table in the Lakehouse.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b","normalized_state":"finished","queued_time":"2024-10-07T03:52:17.3563407Z","session_start_time":null,"execution_start_time":"2024-10-07T03:52:17.9208201Z","execution_finish_time":"2024-10-07T03:52:43.663666Z","parent_msg_id":"3ff39446-2c68-4e49-8a64-9164aa68afc4"},"text/plain":"StatementMeta(, 4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data successfully written to the Delta table in the Lakehouse.\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ddf5b21-cae6-43d5-b77a-a6763d136891"},{"cell_type":"code","source":["# Step 1: Set up the Spark session\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"Fabric_Lakehouse_Demo\").getOrCreate()\n","\n","# Step 2: Define the Lakehouse table path (Delta Table)\n","lakehouse_table_path = \"abfss://fabric@onelake.fabric/Lake.Lakehouse/Tables/Sample\"\n","\n","# Step 3: Read the data from the Delta table\n","df = spark.read.format(\"delta\").load(lakehouse_table_path)\n","\n","# Step 4: Display the first 5 rows of the dataset\n","df.show(5)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b","normalized_state":"finished","queued_time":"2024-10-07T03:53:22.680168Z","session_start_time":null,"execution_start_time":"2024-10-07T03:53:23.3887032Z","execution_finish_time":"2024-10-07T03:53:28.5321504Z","parent_msg_id":"30312293-60ae-4dff-8eb0-a501efbee9c1"},"text/plain":"StatementMeta(, 4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+------------------+---+---------+\n| id|              name|age|     city|\n+---+------------------+---+---------+\n|  1|          John Doe|100| New York|\n|  2|Enrico van de Laar| 42|  Utrecht|\n|  3|     Tomaz Kastrun| 45|Ljubljana|\n+---+------------------+---+---------+\n\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7cac6a79-c07b-4057-ab09-1cc835ff5fca"},{"cell_type":"markdown","source":["Save the results also to a file"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5e96791-c4b2-4116-8408-dd0fd56d5957"},{"cell_type":"code","source":["# Step 5: Define the file path in Files section\n","output_path = \"Files/sample_data.csv\"\n","\n","# Step 6: Save DataFrame as a CSV file\n","df.coalesce(1).write.option(\"header\", \"true\").csv(output_path)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b","normalized_state":"finished","queued_time":"2024-10-07T03:56:13.6115238Z","session_start_time":null,"execution_start_time":"2024-10-07T03:56:14.1234999Z","execution_finish_time":"2024-10-07T03:56:17.8082363Z","parent_msg_id":"d3d57a1b-6a22-4f1e-b261-cd46ce2427bf"},"text/plain":"StatementMeta(, 4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3cf61409-2f1d-409d-96d4-9e73814df5a5"},{"cell_type":"markdown","source":["## Exercise 2: Create a Simple Data Pipeline"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6d6a05f-0720-4d11-a4de-f95ab228047e"},{"cell_type":"markdown","source":["In this exercise, you will create a basic Data Pipeline in Microsoft Fabric using Notebooks and Pipelines. You will extract data from a source, transform it using PySpark, and then load it back into a Lakehouse.\n","\n","Steps:\n","\n","1) Create a Data Pipeline: Set up a data pipeline in Microsoft Fabric.\n","2) Extract: Read data from an external source (e.g., CSV or database) and load it into the Data Lake.\n","3) Transform: Perform some simple data transformation using PySpark (e.g., filtering and aggregation).\n","4) Load: Write the transformed data back to your Lakehouse in Delta format."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bdec97f2-bba2-48d8-9ddb-863bdfa1dca8"},{"cell_type":"code","source":["# Step 1: Read data from a source (e.g., CSV)\n","source_path = \"aabfss://fabric@onelake.fabric/Lake.Lakehouse/Files/sample_data.csv\"\n","df = spark.read.csv(source_path, header=True, inferSchema=True)\n","\n","# Step 2: Perform transformation (e.g., Filter rows where 'age' > 90)\n","filtered_df = df.filter(df[\"Age\"] > 90)\n","\n","# Step 3: Write the transformed data back to the Lakehouse as Delta Table\n","filtered_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://fabric@onelake.fabric/Lake.Lakehouse/Tables/Sample_filtered\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b","normalized_state":"finished","queued_time":"2024-10-07T03:57:16.9691453Z","session_start_time":null,"execution_start_time":"2024-10-07T03:57:17.4694809Z","execution_finish_time":"2024-10-07T03:57:20.0255755Z","parent_msg_id":"5962e1bf-d4af-4cd7-b4eb-974a5c79b8ae"},"text/plain":"StatementMeta(, 4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a85430a1-9e6c-4a23-9e67-0170436cbff1"},{"cell_type":"markdown","source":["Query the table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26c460d6-44ec-4682-8622-9ba259bd329b"},{"cell_type":"code","source":["%%sql\n","\n","SELECT * FROM Sample_filtered"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b","normalized_state":"finished","queued_time":"2024-10-07T03:58:01.1145572Z","session_start_time":null,"execution_start_time":"2024-10-07T03:58:01.5803184Z","execution_finish_time":"2024-10-07T03:58:18.2159221Z","parent_msg_id":"5fa73eb4-6bda-43ed-9198-d9dbbb45df7c"},"text/plain":"StatementMeta(, 4f566dd5-c5d5-41ae-a6d0-0eb8f938d69b, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":7,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"id","type":"integer","nullable":true,"metadata":{}},{"name":"name","type":"string","nullable":true,"metadata":{}},{"name":"age","type":"integer","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}}]},"data":[[1,"John Doe",100,"New York"]]},"text/plain":"<Spark SQL result set with 1 rows and 4 fields>"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"1a126e3c-0309-4127-bc7a-713f03c7c51e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"8254f032-6f1c-4f47-ad3a-f1ec5019be2f","default_lakehouse_name":"LK_flights","default_lakehouse_workspace_id":"3db7091c-9d93-489e-930b-f676a1179736"}}},"nbformat":4,"nbformat_minor":5}