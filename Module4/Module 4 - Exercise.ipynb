{"cells":[{"cell_type":"markdown","source":["# Module 4 - Excercise overview"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"56e71df3-540a-4d88-9f1a-13d54eca306c"},{"cell_type":"markdown","source":["### Fabric Prerequistis\n","\n","You need to have Lakehouse enabled and connected. \n","\n","Link to Lakehouse (replace these strings)\n","- Tables: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables`\n","- Files: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Files`\n","\n","You will also need:\n","- PySpark notebook and connect it to the Fabric standard session"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c32cc80a-ba83-4b60-8f59-657d87539522"},{"cell_type":"markdown","source":["## Exercise 1: Data Analysis and Exploration Using PySpark"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3d5cae90-6e36-41e7-ac49-6fce84998174"},{"cell_type":"markdown","source":["In this exercise, you will perform an exploratory data analysis (EDA) using PySpark in a Microsoft Fabric Notebook. You will load a dataset, explore the data, and generate descriptive statistics, visualizations, and insights.\n","\n","Detailed Instructions:\n","\n","1) Load Data:\n","    - In Microsoft Fabric, create a Lakehouse and load a dataset (e.g., a CSV file with sales or customer data).\n","    - Use PySpark to load the data into a DataFrame.\n","\n","2) Perform Data Exploration:\n","    - Use PySpark to explore the data, including checking for missing values, calculating basic statistics, and visualizing relationships between variables."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"050afa6a-4000-44cf-b651-01f7015603ab"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, isnan, when, count\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DataExploration\").getOrCreate()\n","\n","# Load the dataset from the Lakehouse (Assume sales data)\n","data_path = \"Files/Users/sales_data.csv\"\n","df = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Step 1: Display the first few rows of the dataset\n","df.show(5)\n","\n","# Step 2: Check for missing values in the dataset\n","df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n","\n","# Step 3: Generate descriptive statistics for numeric columns\n","df.describe().show()\n","\n","# Step 4: Perform simple visualizations (example: group sales by region)\n","df.groupBy(\"region\").sum(\"sales_amount\").show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a83f513-9a10-4d15-ab63-dd464b4b95bd"},{"cell_type":"markdown","source":["## Exercise 2: Data Preparation for Machine Learning Model"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a64112e0-5401-47a0-9da4-216e4facb0b5"},{"cell_type":"markdown","source":["In this exercise, you'll prepare a dataset for machine learning. This involves data cleaning, feature engineering, and splitting the dataset into training and testing sets.\n","Step-by-Step Instructions:\n","\n","1) Load and Explore the Data:\n","    - Load the dataset into a PySpark DataFrame and perform initial exploration.\n","\n","2) Clean the Data:\n","    - Handle missing values by imputing or removing them.\n","    - Convert categorical variables into numerical features using techniques like one-hot encoding.\n","\n","3) Feature Engineering:\n","    - Create new features (e.g., time-based features like day of the week, month, etc.).\n","\n","4) Split Data into Training and Testing Sets:\n","    - Use PySpark’s built-in functionality to split the data into training and testing sets."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"92a74a53-ffe3-4e41-8a88-3feb616cce94"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, isnan, when, count, year, month\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DataPreparation\").getOrCreate()\n","\n","# Load the dataset\n","data_path = \"Files/Users/sales_data.csv\"\n","df = spark.read.csv(data_path, header=True, inferSchema=True)\n","\n","# Step 1: Clean Data (Handle missing values by removing rows with missing 'sales_amount')\n","df_clean = df.na.drop(subset=[\"sales_amount\"])\n","\n","# Step 2: Feature Engineering (Example: Extract year and month from 'date' column)\n","df_clean = df_clean.withColumn(\"year\", year(col(\"date\"))).withColumn(\"month\", month(col(\"date\")))\n","\n","# Step 3: One-hot encode the 'region' column\n","indexer = StringIndexer(inputCol=\"region\", outputCol=\"region_index\")\n","encoder = OneHotEncoder(inputCol=\"region_index\", outputCol=\"region_encoded\")\n","\n","# Step 4: Assemble all features into a feature vector\n","assembler = VectorAssembler(inputCols=[\"year\", \"month\", \"region_encoded\"], outputCol=\"features\")\n","\n","# Step 5: Split the data into training and test sets\n","train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n","\n","# Pipeline to handle transformations\n","pipeline = Pipeline(stages=[indexer, encoder, assembler])\n","pipeline_model = pipeline.fit(train_df)\n","train_df_transformed = pipeline_model.transform(train_df)\n","test_df_transformed = pipeline_model.transform(test_df)\n","\n","train_df_transformed.select(\"features\", \"sales_amount\").show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"14b6134f-3199-4e26-a8ca-e08adf056863"},{"cell_type":"markdown","source":["## Exercise 3: Build and Train a Machine Learning Model and Run Experiments"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae8bf4a2-6b25-4bcd-9f73-945ede98eb2e"},{"cell_type":"markdown","source":["In this exercise, you will use the prepared dataset to build a machine learning model (e.g., linear regression) and evaluate its performance. You will also run experiments to tune model hyperparameters.\n","Step-by-Step Instructions:\n","\n","1) Build and Train a Machine Learning Model:\n","    - Use the transformed dataset from the previous exercise to build and train a Linear Regression model using PySpark’s MLlib.\n","\n","2) Run Experiments:\n","    - Perform hyperparameter tuning using GridSearch or Cross-Validation to find the best-performing model.\n","\n","3) Evaluate the Model:\n","    - Evaluate the model on the test set using performance metrics like Mean Squared Error (MSE) or R-squared."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03b8850a-006c-4309-8f5d-375fddc895e2"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"ML_Model_Training\").getOrCreate()\n","\n","# Assuming 'train_df_transformed' and 'test_df_transformed' from Exercise 2\n","\n","# Step 1: Build and Train a Linear Regression Model\n","lr = LinearRegression(featuresCol=\"features\", labelCol=\"sales_amount\")\n","\n","# Step 2: Set up Grid Search for Hyperparameter Tuning\n","paramGrid = ParamGridBuilder() \\\n","    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n","    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n","    .build()\n","\n","# Step 3: Cross-Validation setup\n","evaluator = RegressionEvaluator(labelCol=\"sales_amount\", metricName=\"rmse\")\n","crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n","\n","# Step 4: Train the model using Cross-Validation\n","cv_model = crossval.fit(train_df_transformed)\n","\n","# Step 5: Evaluate the model on the test set\n","predictions = cv_model.transform(test_df_transformed)\n","rmse = evaluator.evaluate(predictions)\n","r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n","\n","print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","print(f\"R-Squared: {r2}\")\n","\n","# Show some sample predictions\n","predictions.select(\"features\", \"sales_amount\", \"prediction\").show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ce482089-548f-46ca-b9e3-1e6a161c732d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}