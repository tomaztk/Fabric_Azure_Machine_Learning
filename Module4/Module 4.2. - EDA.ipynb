{"cells":[{"cell_type":"markdown","source":["# Module 4.2. - EDA\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f9731632-b0e4-4056-8fe5-e17150cd85f2"},{"cell_type":"markdown","source":["### Fabric Prerequistis\n","\n","You need to have Lakehouse enabled and connected. \n","\n","Link to Lakehouse (replace these strings)\n","- Tables: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables`\n","- Files: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Files`\n","\n","You will also need:\n","- PySpark notebook and connect it to the Fabric standard session\n","\n","Data:\n","- Have delta tables created with flights data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"673cf0ea-b6d3-4dd2-bb4b-f92a2aa400d8"},{"cell_type":"markdown","source":["In this module, you will analyze flight data, perform simple Exploratory Data Analysis (EDA), and build a machine learning model to predict whether an upcoming flight will be delayed. You will store and retrieve data from a Lakehouse, build a prediction model using PySpark, and run experiments to evaluate the model's performance."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"39202121-e09b-4702-91ed-06f81a559717"},{"cell_type":"markdown","source":["We'll go through these steps:\n","\n","1) Perform Simple EDA using PySpark notebooks.\n","2) Explore and understand the flight data stored in the Lakehouse.\n","3) Prepare and build a machine learning model that predicts flight delays.\n","3) Train the model and run experiments.\n","4) Switch between PySpark, SQL, and R to show how different languages can be used in the notebook."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"644460c1-156f-4775-aff2-15edd7879f6a"},{"cell_type":"markdown","source":["## Step 1: Load Flight Data from the Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"923b6324-7aa8-4fa8-a795-025bc7ccf665"},{"cell_type":"code","source":["df_csv = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/flights.csv\")\n","\n","#####\n","# or\n","#####\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"CSVtoDelta\").getOrCreate()\n","\n","csv_file_path = \"Files/flights.csv\"\n","\n","\n","df_csv = spark.read.format(\"csv\") \\\n","    .option(\"header\", \"true\") \\  \n","    .option(\"inferSchema\", \"true\") \\  \n","    .load(csv_file_path)\n","\n","\n","df_csv.show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4966204d-53f4-45b5-aef4-4b94521fc1a2"},{"cell_type":"markdown","source":["And add it to the delta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef18ddb9-74fc-4873-8f26-04de2fe39d92"},{"cell_type":"code","source":["# Step 1: Define the path in the Lakehouse where the Delta table will be saved\n","delta_table_path = \"Tables/flights_table\"\n","\n","# Step 7: Save the DataFrame as a Delta table in the Lakehouse\n","df_csv.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f43dcc3f-cd4c-4164-b3d0-0bdb838d981a"},{"cell_type":"code","source":["# Step 1: Initialize Spark session\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"FlightDelayEDA\").getOrCreate()\n","\n","# Step 2: Define the Lakehouse path to the Delta Table\n","lakehouse_table_path = \"abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables/nyctaxi_prep\"\n","\n","# Step 3: Read the flight data from Delta Table in Lakehouse\n","df_flight = spark.read.format(\"delta\").load(lakehouse_table_path)\n","\n","# Step 4: Display the first few rows of the flight data\n","df_flight.show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8289fd23-ccba-4e1b-926a-1d9e4c649e5e"},{"cell_type":"markdown","source":["Alternatively, you can use SQL within the notebook:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3cb552c8-e71e-46a8-8f1b-a9eaa128ed20"},{"cell_type":"code","source":["%%sql\n","\n","SELECT * FROM delta.`abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables/nyctaxi_prep` LIMIT 5;\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"12960c96-e0e6-41f1-9bd5-20838cb6292f"},{"cell_type":"markdown","source":["Or use R for the same task:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6c8aa62b-ab0e-4e5e-b2b4-6113a5892681"},{"cell_type":"code","source":["%%sparkr\n","\n","library(SparkR)\n","\n","df_flight <- read.df(\"abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables/nyctaxi_prep\", source = \"delta\")\n","head(df_flight)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"r","language_group":"synapse_pyspark"}},"id":"b695fc3b-201d-487e-9929-b7048b33f51d"},{"cell_type":"markdown","source":["## Step 2: Simple Exploratory Data Analysis (EDA)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b98be949-52de-4119-9531-9bac76d04ff2"},{"cell_type":"markdown","source":["Check the Schema:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00a204ee-0cbd-42d0-ba6c-20544d7c7016"},{"cell_type":"code","source":["# Display the schema of the flight data\n","df_flight.printSchema()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"082fe913-9abe-407e-b626-4eee3408135b"},{"cell_type":"markdown","source":["Summary Statistics:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"701a3a46-f97f-4aba-8d71-8d998047ffd4"},{"cell_type":"code","source":["# Display summary statistics for all numerical columns\n","df_flight.describe().show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d32b49f7-6ea5-4865-8009-edf98ea6db67"},{"cell_type":"markdown","source":["Check for Missing Data:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4e708b7-9330-4686-8812-00fe9f716b27"},{"cell_type":"code","source":["from pyspark.sql.functions import col, count, when\n","\n","# Check for missing data by counting null values in each column\n","missing_data = df_flight.select([count(when(col(c).isNull(), c)).alias(c) for c in df_flight.columns])\n","missing_data.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dea15ecb-d980-4c8d-8f64-b3bae7707f21"},{"cell_type":"markdown","source":["Distribution of Flight Delays:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8b6a3ac-5e4a-451d-876b-b38a307094dd"},{"cell_type":"code","source":["# Group by the 'is_delay' column to check the distribution of delayed vs non-delayed flights\n","df_flight.groupBy(\"is_delay\").count().show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28786b3b-6460-47b7-b6ef-004b1ddf028d"},{"cell_type":"markdown","source":["And we can  explore data with SQL for aggregation:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fdb6771-5d24-4603-8d59-67001b486da4"},{"cell_type":"code","source":["%%sql\n","-- Count the number of delayed and non-delayed flights\n","\n","SELECT is_delay, COUNT(*) AS count \n","FROM delta.`abfss://<your-container>@<your-storage-account>.dfs.core.windows.net/delta/flight_data`\n","GROUP BY is_delay;\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"}},"id":"a0ee3303-e887-4020-b798-d585d791c79a"},{"cell_type":"markdown","source":["## Step 3: Data Preparation for Machine Learning"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6030ed4-fbf5-4beb-8219-4ca4953bda12"},{"cell_type":"markdown","source":["We will prepare the flight data for building the machine learning model. We'll transform the features and prepare the dataset for training."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c883a6d7-461b-47e6-b282-5250a7f5f8f8"},{"cell_type":"markdown","source":["### Feature Selection:\n","\n","We'll use key features such as departure time, origin, destination, and carrier to predict the delay."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e19b7811-ccd7-47a5-ba14-2a647d30a264"},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\n","\n","# Index categorical columns (e.g., carrier, origin, destination)\n","indexer_carrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrier_index\")\n","indexer_origin = StringIndexer(inputCol=\"origin\", outputCol=\"origin_index\")\n","indexer_dest = StringIndexer(inputCol=\"destination\", outputCol=\"dest_index\")\n","\n","df_flight = indexer_carrier.fit(df_flight).transform(df_flight)\n","df_flight = indexer_origin.fit(df_flight).transform(df_flight)\n","df_flight = indexer_dest.fit(df_flight).transform(df_flight)\n","\n","# Combine all relevant features into a single vector column\n","assembler = VectorAssembler(inputCols=[\"carrier_index\", \"origin_index\", \"dest_index\", \"departure_time\"], outputCol=\"features\")\n","df_flight = assembler.transform(df_flight)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64a2fa57-40b9-43a3-bf93-08945d5d457b"},{"cell_type":"markdown","source":["## Step 4: Build and Train a Machine Learning Model"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d69aeb1-6af8-45e9-aaa6-ad12b4bec8b6"},{"cell_type":"markdown","source":["Build a Logistic Regression model to predict whether a flight will be delayed or not.\n","\n","1) Train-Test Split:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"feead500-2ba9-444f-9137-de4c60f38839"},{"cell_type":"code","source":["# Split the data into training (80%) and test (20%) sets\n","train_df, test_df = df_flight.randomSplit([0.8, 0.2], seed=42)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"16d51d87-a0bd-4edd-b147-157af7812ce7"},{"cell_type":"markdown","source":["2) Logistic Regression Model:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"64ff1948-ff92-4442-990f-acbe55cf7a9e"},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","\n","# Initialize the Logistic Regression model\n","lr = LogisticRegression(featuresCol=\"features\", labelCol=\"is_delay\")\n","\n","# Train the model\n","lr_model = lr.fit(train_df)\n","\n","# Display the model summary\n","lr_model.summary\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dad83477-65db-488b-a9fc-a4be89215aed"},{"cell_type":"markdown","source":["## Step 5: Run Experiments and Evaluate the Model"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1162f142-dcd1-455a-8fec-67b03bde198d"},{"cell_type":"markdown","source":["We will evaluate the performance of the trained model using metrics such as accuracy and Area Under the ROC Curve (AUC).\n","\n","1) Make Predictions:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09e42bfa-7870-4fcd-ae28-91b40be742c2"},{"cell_type":"code","source":["# Use the model to make predictions on the test data\n","predictions = lr_model.transform(test_df)\n","\n","# Show the predictions\n","predictions.select(\"features\", \"is_delay\", \"prediction\", \"probability\").show(5)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d2925225-95fb-4089-8a94-fb6136a8c183"},{"cell_type":"markdown","source":["2) Evaluate the Model:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"858914a6-59ae-44ed-bdcd-cc55b69f89b4"},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","# Initialize the evaluator for AUC metric\n","evaluator = BinaryClassificationEvaluator(labelCol=\"is_delay\", metricName=\"areaUnderROC\")\n","\n","# Calculate AUC for the test dataset\n","auc = evaluator.evaluate(predictions)\n","print(f\"AUC: {auc}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f2d67849-e205-4d84-bb7b-e15dfa186d1f"},{"cell_type":"markdown","source":["Alternatively, you can calculate accuracy using SQL:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40c8fe89-2243-467f-8dca-f14bbe03cfdc"},{"cell_type":"code","source":["%%sql\n","\n","-- Calculate accuracy of the model\n","SELECT COUNT(*) * 1.0 / SUM(CASE WHEN prediction = is_delay THEN 1 ELSE 0 END) AS accuracy\n","FROM predictions;\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"}},"id":"f82463d5-266b-40be-986d-f5af3064cd92"},{"cell_type":"markdown","source":["## Step 6: Track Experiments and Run Multiple Models"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96c4b5f8-f7a6-4c02-bbfd-92c8daaa92e2"},{"cell_type":"markdown","source":["We will Track the model’s performance across different experiments and retrain if necessary.\n","\n","Using MLFlow in Microsoft Fabric allows you to track and compare multiple model versions easily."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1ea6920f-ff0f-4369-b56b-e8926fb8e000"},{"cell_type":"code","source":["import mlflow\n","import mlflow.spark\n","\n","# Start MLflow experiment tracking\n","mlflow.start_run()\n","\n","# Log parameters, metrics, and the model\n","mlflow.log_param(\"model\", \"LogisticRegression\")\n","mlflow.log_metric(\"AUC\", auc)\n","mlflow.spark.log_model(lr_model, \"flight_delay_model\")\n","\n","# End MLflow run\n","mlflow.end_run()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"901be66e-70fe-4cc9-8d34-b96dd7c7cffd"},{"cell_type":"markdown","source":["## Step 7: Making Predictions for the Next Flight (Inference)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f821b189-c589-4c25-a3ba-b09d160ae4a5"},{"cell_type":"markdown","source":["Perform inference for upcoming flights to predict whether they will be delayed."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e90e9d20-e1e2-4137-8cef-834b2f1edb84"},{"cell_type":"code","source":["# Assuming new_flight_df contains data for an upcoming flight\n","new_flight_df = spark.createDataFrame([\n","    (\"AA\", \"LAX\", \"JFK\", \"16:00\")\n","], [\"carrier\", \"origin\", \"destination\", \"departure_time\"])\n","\n","# Prepare the new data (index and vectorize)\n","new_flight_df = indexer_carrier.fit(new_flight_df).transform(new_flight_df)\n","new_flight_df = indexer_origin.fit(new_flight_df).transform(new_flight_df)\n","new_flight_df = indexer_dest.fit(new_flight_df).transform(new_flight_df)\n","\n","new_flight_df = assembler.transform(new_flight_df)\n","\n","# Make the prediction\n","new_flight_prediction = lr_model.transform(new_flight_df)\n","\n","# Show the prediction\n","new_flight_prediction.select(\"features\", \"prediction\", \"probability\").show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"075e93c2-651b-42f6-8cc8-375f0b9b8395"},{"cell_type":"markdown","source":["## Switching to SQL or R"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b7e2647-97f3-41ce-a13b-8b7268878b78"},{"cell_type":"markdown","source":["We can  also make predictions using SQL if the model has been registered in MLFlow:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78be68d1-7ce3-4b75-acdb-7886de0a910a"},{"cell_type":"code","source":["%%sql\n","SELECT *, PREDICT(flight_delay_model, carrier_index, origin_index, dest_index, departure_time)\n","FROM delta.`abfss://<your-container>@<your-storage-account>.dfs.core.windows.net/delta/new_flight_data`;\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"}},"id":"94bbf1a1-f8bd-4e40-a7c5-3473ae736e84"},{"cell_type":"markdown","source":["Or use R for Training the Model:"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d8b42616-0650-45ce-990d-91f910fc991e"},{"cell_type":"code","source":["library(SparkR)\n","\n","# Split the dataset into training and testing sets\n","splits <- randomSplit(df_flight, c(0.8, 0.2), seed = 42)\n","train_df <- splits[[1]]\n","test_df <- splits[[2]]\n","\n","# Train a Logistic Regression model in R\n","lr_model <- spark.logit(train_df, label = \"is_delay\", features = c(\"carrier_index\", \"origin_index\", \"dest_index\", \"departure_time\"))\n","\n","# Make predictions and evaluate the model\n","predictions <- predict(lr_model, test_df)\n","head(predictions)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b882d2d1-e7da-4441-bcbd-583a4a8ddece"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"8254f032-6f1c-4f47-ad3a-f1ec5019be2f","default_lakehouse_name":"LK_flights","default_lakehouse_workspace_id":"3db7091c-9d93-489e-930b-f676a1179736"}}},"nbformat":4,"nbformat_minor":5}