{"cells":[{"cell_type":"markdown","source":["# Module 2 - Excercise overview\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3d80ab1e-6f90-4205-80ca-90c15ae61309"},{"cell_type":"markdown","source":["### Fabric Prerequistis\n","\n","You need to have Lakehouse enabled and connected. \n","\n","Link to Lakehouse (replace these strings)\n","- Tables: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Tables`\n","- Files: `abfss://Fabric_2024@onelake.dfs.fabric.microsoft.com/LK_flights.Lakehouse/Files`\n","\n","You will also need:\n","- PySpark notebook and connect it to the Fabric standard session\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6471910f-4b45-440f-841a-ab71e14a7ce9"},{"cell_type":"markdown","source":["## Exercise 1: Creating a Lakehouse and Writing Data to a Delta Table\n","\n","In this exercise, you'll create a storage resource (Lakehouse) in Microsoft Fabric and store structured data in a Delta Table using PySpark."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a4594e3e-9fda-457d-a71e-0e976a69d0d6"},{"cell_type":"markdown","source":["### Step-by-Step Instructions:\n","\n","1) Create a Lakehouse in Microsoft Fabric:\n","    - Go to your Microsoft Fabric workspace.\n","    - Create a Lakehouse storage option from the \"New\" menu. The Lakehouse provides a scalable data lake storage.\n","    - This Lakehouse will serve as your storage layer.\n","\n","2) Load Data into PySpark DataFrame:\n","    - In Microsoft Fabric's notebook or PySpark environment, load a dataset into a PySpark DataFrame. In this example, we'll use some sample data (a dictionary) and convert it to a PySpark DataFrame.\n","\n","3) Write Data to Delta Table:\n","    - After creating the DataFrame, save it as a Delta table in your Lakehouse storage."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4bcf80f0-0d2c-48ab-a06a-5cb716200a48"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","\n","\n","spark = SparkSession.builder.appName(\"LakehouseExample\").getOrCreate()\n","\n","\n","data = [\n","    Row(id=1, name='John Doe', age=100, city='New York'),\n","    Row(id=2, name='Enrico van de Laar', age=42, city='Utrecht'),\n","    Row(id=3, name='Tomaz Kastrun', age=45, city='Ljubljana')\n","]\n","df = spark.createDataFrame(data)\n","\n","# Write DataFrame to Delta Table in the Lakehouse\n","lakehouse_path = \"Files/Files/Files\"  # Modify path according to your Lakehouse structure\n","df.write.format(\"delta\").mode(\"overwrite\").save(lakehouse_path)\n","\n","print(\"Data successfully written to the Delta table in the Lakehouse.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3bca66c5-20c8-4358-9c1b-7c77848175d8"},{"cell_type":"markdown","source":["## Exercise 2: Creating a Spark Cluster for Distributed Computation"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fb24739-1294-4330-94a7-bc236de446b3"},{"cell_type":"markdown","source":["In this exercise, you will create a compute resource (Spark Cluster) in Microsoft Fabric and perform distributed data processing using PySpark.\n","Step-by-Step Instructions:\n","\n","1) Create a Spark Cluster in Microsoft Fabric:\n","    - Go to your Microsoft Fabric workspace and choose \"Spark Job Definition\" from the \"New\" menu to create a Spark Cluster.\n","    - Set the cluster size depending on the data size and computational needs.\n","\n","2) Perform Distributed Computation:\n","    - Once the Spark Cluster is available, use PySpark to load a dataset and perform distributed computation such as calculating averages or sums."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"97803d03-cb00-4fd2-8e11-b4ea0fe05e4b"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"DistributedComputation\").getOrCreate()\n","\n","# Load data into PySpark DataFrame\n","data = [(1, 'Math', 85),\n","        (2, 'Math', 90),\n","        (3, 'Math', 78),\n","        (1, 'Science', 95),\n","        (2, 'Science', 89),\n","        (3, 'Science', 92)]\n","\n","columns = ['student_id', 'subject', 'score']\n","df = spark.createDataFrame(data, columns)\n","\n","# Perform distributed computation: Calculate average score per subject\n","avg_scores = df.groupBy(\"subject\").avg(\"score\")\n","avg_scores.show()\n","\n","# Optionally, write the output to a Delta Table or storage\n","output_path = \"Files/Files/Files\"\n","avg_scores.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40dcf9e6-f830-43f6-8a97-23b61919d6dd"},{"cell_type":"markdown","source":["### Exercise 3: Partitioning Large Datasets in Storage for Optimized Compute Performance"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"634b5ad0-f9ad-4aef-98b2-c95ac000bc81"},{"cell_type":"markdown","source":["In this exercise, you will store a large dataset with partitioning to improve compute efficiency. Partitioning helps when working with large datasets by splitting them into smaller, more manageable parts.\n","Step-by-Step Instructions:\n","\n","1) Create or Load a Large Dataset:\n","    - You can create or load a large dataset (for this exercise, we simulate one).\n","    - In Microsoft Fabric, partition the dataset based on a key (e.g., year, region) to improve query performance.\n","\n","2) Partition Data in PySpark:\n","    - Use PySpark to partition the DataFrame based on a column (e.g., region).\n","    - Write the partitioned data to Delta format in Lakehouse storage."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"97897725-7a2d-4ba6-b6ab-373db7e051a3"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql import Row\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName(\"PartitionedData\").getOrCreate()\n","\n","# Create a large sample dataset with multiple regions\n","data = [\n","    Row(id=1, name='John Doe', age=28, region='North America'),\n","    Row(id=2, name='Jane Smith', age=34, region='Europe'),\n","    Row(id=3, name='Sam Brown', age=22, region='Asia'),\n","    Row(id=4, name='Linda Green', age=40, region='North America'),\n","    Row(id=5, name='Tom Harris', age=29, region='Europe')\n","    # Add more rows to simulate a large dataset...\n","]\n","\n","df = spark.createDataFrame(data)\n","\n","# Partition the dataset by the 'region' column and write to Delta Table\n","partitioned_path = \"Files/Users/partitioned_data\" # Modify path according to your Lakehouse structure\n","df.write.format(\"delta\").partitionBy(\"region\").mode(\"overwrite\").save(partitioned_path)\n","\n","print(\"Data successfully written to Lakehouse with partitioning by region.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"24686e09-388f-42c6-a8a7-3949434c99a3"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}